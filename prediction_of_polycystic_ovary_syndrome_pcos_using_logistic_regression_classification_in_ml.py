# -*- coding: utf-8 -*-
"""Prediction-of-Polycystic-ovary-syndrome-PCOS-using-Logistic Regression-Classification-in-ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LuptMuSvvVDXIWH3G2XhsJ81MEWSawV6

# Abstract

Polycystic ovary syndrome is a disorder involving infrequent, irregular or prolonged menstrual periods, and often excess male hormone (androgen) levels. The ovaries develop numerous small collections of fluid — called follicles — and may fail to regularly release eggs.

The Dataset contains all physical and clinical parameters to determine PCOS and infertility related issues .

Getting to know our dataset:

* Patient File No. : This is the report number which has data for a particular
 patient
* PCOS : Polycystic ovary syndrome (PCOS) is a hormonal disorder common among women of reproductive age, we would like to determine whether the patient has this syndrome or not
* Age (yrs) : Age of patient in years
* Weight (Kg) : Weight of patient in kg
* Height(Cm) : Height of patient in centimeter
* BMI : Body mass index of the patient
* Blood Group : Blood Group of the patient A+ = 11, A- = 12, B+ = 13, B- = 14, O+ =15, O- = 16, AB+ =17, AB- = 18 (total 8 blood groups)
* Pulse rate(bpm) : It is the heart rate of patient in beats per minute.
* Resting heart rate for adults ranges from 60 to 100 beats per minute
RR (breaths/min) : It is the respiration rate. Normal respiration rates for an adult person at rest range from 12 to 16 breaths per minute.
* Hb(g/dl) : Hemoglobin levels in gram per deciliter. For women, a normal level ranges between 12.3 gm/dL and 15.3 gm/dL.
* Cycle length(days) : This represents length of menstrual cycle. The length of the menstrual cycle varies from woman to woman, but the average is to have periods every 28 days.
* Marraige Status (Yrs) : Years of marriage
* Pregnant(Y/N) : If the patient is pregnant
* No. of aborptions : No. of aborptions, if any. There are total 541 values out of which 437 patients never had any abortions.
* I beta-HCG(mIU/mL) : this is case 1 of beta hcg
* II beta-HCG(mIU/mL) : this is case 2 of beta hcg (please note: An beta hCG level of less than 5 mIU/mL is considered negative for pregnancy, and anything above 25 mIU/mL is considered positive for pregnancy) (also the unit mIU/mL is mili International Units per miliLiter)
* FSH(mIU/mL) : Its full form is Follicle-stimulating hormone. During puberty: it ranges from 0.3 to 10.0 mIU/mL (0.3 to 10.0 IU/L) Women who are still menstruating: 4.7 to 21.5 mIU/mL (4.5 to 21.5 IU/L) After menopause: 25.8 to 134.8 mIU/mL (25.8 to 134.8 IU/L)
* LH(mIU/mL) : It is Luteinizing Hormone.
* FSH/LH : Ratio of FSH and LH
* Hip(inch) : Hip size in inches
* Waist(inch) : Waist Size in inches
* Waist:Hip Ratio : Waist by hip ratio
* TSH (mIU/L) : It is thyroid stimulating hormone. Normal values are from 0.4 to 4.0 mIU/L
* AMH(ng/mL) : It is Anti-Mullerian Hormone.
* PRL(ng/mL) : This represents Prolactin levels.
* Vit D3 (ng/mL): Vitamin D levels. Normal vitamin D levels in the blood are 20 ng/ml or above for adults.
* PRG(ng/mL): Progesterone levels
* RBS(mg/dl): This value is obtained by doing Random Blood Sugar (RBS) Test.
* Weight gain(Y/N): Is there been a weight gain
* hair growth(Y/N): Is there been a hair growth
* Skin darkening (Y/N): Skin darkening issues
* Hair loss(Y/N): hair loss issues
* Pimples(Y/N): pimples issues
* Fast food (Y/N): is fast food part of you diet
* Reg.Exercise(Y/N): do you do exercises on a regular basis
* BP _Systolic (mmHg): Systolic blood pressure, measures the pressure in your arteries when your heart beats.
* BP _Diastolic (mmHg): Diastolic blood pressure, measures the pressure in your arteries when your heart rests between beats.
* Follicle No. (L): Follicles number in the left side
* Follicle No. (R): Follicles number in the right side
* Avg. F size (L) (mm): Average Follicle size in the left side in mm
* Avg. F size (R) (mm): Average Follicle size in the right side in mm
* Endometrium (mm): Size of Endometrium in mm
"""

# Import Libraries

# Data Manipulation
!pip install fitter
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import MinMaxScaler
from fitter import Fitter, get_common_distributions, get_distributions
import scipy.stats as stats

# Data Visualization
!pip install shap
import seaborn as sns
import matplotlib.pyplot as plt
import shap  # v0.39.0

shap.initjs()

# Figure Size
plt.figure(figsize=(18, 14), dpi=80)
sns.set(rc={"figure.figsize": (18, 14)})

# Data Imputation
!pip install fancyimpute
from fancyimpute import IterativeImputer as MICE
from fancyimpute import KNN

# Modeling
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor
from sklearn.inspection import permutation_importance

##Installing all the required libraries for AutoML Modelling
# !pip install requests
# !pip install tabulate
# !pip install "colorama>=0.3.8"
# !pip install future
# !pip install h2o

# Importing all the libararies required for AutoML Modelling
# import h2o
# from h2o.automl import H2OAutoML
# from h2o.estimators.gbm import H2OGradientBoostingEstimator
# from h2o.grid.grid_search import H2OGridSearch
# import random, os, sys
# from datetime import datetime
# import logging
# import csv
# import optparse
# import time
import json
from distutils.util import strtobool
import psutil
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns

sns.set(rc={"figure.figsize": (16, 8)})

"""IMPORTING THE DATASET FROM GITHUB.

The Dataset has 2 csv - PCOS Patients with Fertility and PCOS with Infertility.

Both the CSVs have been fetched and merged.
"""

df_inf = pd.read_csv("https://raw.githubusercontent.com/Muskansri1/Prediction-of-Polycystic-ovary-syndrome-PCOS-using-Classification-in-ML/main/PCOD_Data/PCOS_infertility.csv")

df_inf.head()

df_wo_infertility = pd.read_excel("https://github.com/Muskansri1/Prediction-of-Polycystic-ovary-syndrome-PCOS-using-Classification-in-ML/blob/main/PCOD_Data/PCOS_data_without_infertility.xlsx?raw=true",sheet_name="Full_new")

df_wo_infertility.head()

df_inf.columns

df_wo_infertility.columns

#The Dataset has 2 CSVs - PCOS Patients with Fertility and PCOS Patients without Infertility.
#Merging the datasets together based on the patient
#delete repeated features and change PCOS(Y/N) to Target.

#Merge the files
data = pd.merge(df_wo_infertility,df_inf, on='Patient File No.', suffixes={'','_wo'},how='left')
#Dropping duplicate features
data =data.drop(['Unnamed: 44', 'Sl. No_wo', 'PCOS (Y/N)_wo', '  I   beta-HCG(mIU/mL)_wo',
       'II    beta-HCG(mIU/mL)_wo', 'AMH(ng/mL)_wo'], axis=1)
#Change the target variable
data = data.rename(columns = {"PCOS (Y/N)":"Target"})

data.head()

#Drop unnecessary features
data = data.drop(["Sl. No","Patient File No."],axis = 1)

data.info()

#We can see from the above columns that the following columns are categorical/numerical
#Categorical Variable : Target, Pregnant(Y/N), Weight gain(Y/N), hair growth(Y/N), Skin darkening (Y/N), Hair loss(Y/N), Pimples(Y/N), Fast food (Y/N), Reg.Exercise(Y/N), Blood Group
#Numerical Variable : Age (yrs), Weight (Kg),Marraige Status (Yrs)

data = data.rename(columns=lambda x: x.strip()) #trimming column names where there are extra spaces at start and end

data.head()

#Converting the Object Datatype to Numerical Datatype - If ‘coerce’, then invalid parsing will be set as NaN.
data["AMH(ng/mL)"] = pd.to_numeric(data["AMH(ng/mL)"], errors='coerce')
data["II    beta-HCG(mIU/mL)"] = pd.to_numeric(data["II    beta-HCG(mIU/mL)"], errors='coerce')

"""# CHECKING FOR MISSING VALUES

**QUESTION 2**: Are there missing values?

**ANSWER 2:** Yes, there exist missing data in Marraige Status (Yrs)', 'II    beta-HCG(mIU/mL)', 'AMH(ng/mL)',
       'Fast food (Y/N)
       
It has been shown below. HeatMap is also been plot to visualize the missing values

"""

data.isnull().sum()

#We can see that the missing data exists in Marraige Status (Yrs), II    beta-HCG(mIU/mL), AMH(ng/mL), Fast food (Y/N) - We Would be resolving them in later stages using Imputations

data.columns[data.isnull().any()]

#plotting heatmap to visualize missing values
plt.figure(figsize=(16,12))
sns.heatmap(data.isna().transpose(),
            cmap="YlGnBu",
            cbar_kws={'label': 'Missing Data'}, xticklabels=True, yticklabels=True)



"""# Splitting the Data Into Train and Test before Imputing Null Values"""

training_data, testing_data = train_test_split(data, test_size=0.2, random_state=0)

print(f"No. of training examples: {training_data.shape[0]}")
print(f"No. of testing examples: {testing_data.shape[0]}")

data.columns

training_data.head()



"""Training and Testing data visualization

**Question 6:** Do the training and test sets have the same data?

**Answer 6**: The Training data and test data are unique, but similar. I have plotted the ecdfplot to show the similarity between the 2 datasets and a function to check the unique data in both the training and test data.
"""

# Return False if Training and Testing data is completely seperate, orelse returns True
check_df = pd.merge(
    training_data,
    testing_data,
    on=[
        'Target','Age (yrs)', 'Weight (Kg)', 'Height(Cm)', 'BMI',
       'Blood Group', 'Pulse rate(bpm)', 'RR (breaths/min)', 'Hb(g/dl)',
       'Cycle(R/I)', 'Cycle length(days)', 'Marraige Status (Yrs)',
       'Pregnant(Y/N)', 'No. of aborptions', 'I   beta-HCG(mIU/mL)',
       'II    beta-HCG(mIU/mL)', 'FSH(mIU/mL)', 'LH(mIU/mL)', 'FSH/LH',
       'Hip(inch)', 'Waist(inch)', 'Waist:Hip Ratio', 'TSH (mIU/L)',
       'AMH(ng/mL)', 'PRL(ng/mL)', 'Vit D3 (ng/mL)', 'PRG(ng/mL)',
       'RBS(mg/dl)', 'Weight gain(Y/N)', 'hair growth(Y/N)',
       'Skin darkening (Y/N)', 'Hair loss(Y/N)', 'Pimples(Y/N)',
       'Fast food (Y/N)', 'Reg.Exercise(Y/N)', 'BP _Systolic (mmHg)',
       'BP _Diastolic (mmHg)', 'Follicle No. (L)', 'Follicle No. (R)',
       'Avg. F size (L) (mm)', 'Avg. F size (R) (mm)', 'Endometrium (mm)',
    ],
    how="left",
    indicator="Exist",
)
check_df.drop("Target", inplace=True, axis=1)
check_df["Exist"] = np.where(check_df.Exist == "both", True, False)
print(
    check_df["Exist"].value_counts()
)  # This code prints the number of unique data rows in training colomns

feature_name = 'Weight (Kg)'

df = pd.DataFrame({
    feature_name:np.concatenate((training_data.loc[:,feature_name],testing_data.loc[:,feature_name])),
    'set':['training']*training_data.shape[0] + ['test']*testing_data.shape[0]
    })
sns.ecdfplot(data=df,x=feature_name,hue='set')

training_data.isnull().sum()

testing_data.isnull().sum()



training_data.columns

"""RENAMING THE COLUMNS FOR BETTER USABILITY OF CODE"""

training_data.rename(columns={'Age (yrs)': 'Age_in_Years', 'Weight (Kg)': 'Weight_in_Kg','Height(Cm)':'Height_IN_Cm','Blood Group': 'BloodGroup','Pulse rate(bpm)': 'PulseRate_BPM','RR (breaths/min)': 'RR_in_Breaths_Per_Min','Hb(g/dl)':'Hb_IN_g_per_dl','Cycle(R/I)':'Cycle','Cycle length(days)': 'CycleLength_In_days','Marraige Status (Yrs)': 'MarraigeStatus_In_Yrs','Pregnant(Y/N)':'Pregnant','No. of aborptions': 'NumOfAborptions','I   beta-HCG(mIU/mL)': 'I_beta_HCG_IN_mIU_per_mL'}, inplace=True)

testing_data.rename(columns={'Age (yrs)': 'Age_in_Years', 'Weight (Kg)': 'Weight_in_Kg','Height(Cm)':'Height_IN_Cm','Blood Group': 'BloodGroup','Pulse rate(bpm)': 'PulseRate_BPM','RR (breaths/min)': 'RR_in_Breaths_Per_Min','Hb(g/dl)':'Hb_IN_g_per_dl','Cycle(R/I)':'Cycle','Cycle length(days)': 'CycleLength_In_days','Marraige Status (Yrs)': 'MarraigeStatus_In_Yrs','Pregnant(Y/N)':'Pregnant','No. of aborptions': 'NumOfAborptions','I   beta-HCG(mIU/mL)': 'I_beta_HCG_IN_mIU_per_mL'}, inplace=True)

training_data.rename(columns={'II    beta-HCG(mIU/mL)': 'II_beta_HCG_IN_mIU_per_mL','FSH(mIU/mL)':'FSH_IN_mIU_per_mL','LH(mIU/mL)':'LH_IN_mIU_per_mL','FSH/LH':'FSH_per_LH','Hip(inch)':'Hip_IN_inch','Waist(inch)':'Waist_IN_inch','Waist:Hip Ratio':'Waist_to_Hip_Ratio','TSH (mIU/L)':'TSH_IN_mIU_per_L','AMH(ng/mL)':'AMH_IN_ng_per_mL','PRL(ng/mL)':'PRL_IN_ng_per_mL','Vit D3 (ng/mL)':'Vit_D3_IN_ng_per_mL','PRG(ng/mL)':'PRG_IN_ng_per_mL','RBS(mg/dl)':'RBS_IN_mg_per_dl','Pimples(Y/N)':'Pimples','Weight gain(Y/N)': 'Weight_gain','hair growth(Y/N)': 'Hair_Growth','Skin darkening (Y/N)': 'Skin_Darkening','Hair loss(Y/N)': 'Hair_loss','Fast food (Y/N)': 'Fast_food','Reg.Exercise(Y/N)': 'Reg_Exercise','BP _Systolic (mmHg)': 'BP_Systolic_IN_mmHg','BP _Diastolic (mmHg)': 'BP_Diastolic_IN_mmHg','Follicle No. (L)':'Follicle_Num_Left','Follicle No. (R)':'Follicle_Num_Right','Avg. F size (L) (mm)':'Avg_F_size_Left_IN_mm','Avg_F_size(R)_(mm)':'Avg_F_size_Right_IN_mm', 'Endometrium (mm)':'Endometrium_IN_mm'}, inplace=True)

testing_data.rename(columns={'II    beta-HCG(mIU/mL)': 'II_beta_HCG_IN_mIU_per_mL','FSH(mIU/mL)':'FSH_IN_mIU_per_mL','LH(mIU/mL)':'LH_IN_mIU_per_mL','FSH/LH':'FSH_per_LH','Hip(inch)':'Hip_IN_inch','Waist(inch)':'Waist_IN_inch','Waist:Hip Ratio':'Waist_to_Hip_Ratio','TSH (mIU/L)':'TSH_IN_mIU_per_L','AMH(ng/mL)':'AMH_IN_ng_per_mL','PRL(ng/mL)':'PRL_IN_ng_per_mL','Vit D3 (ng/mL)':'Vit_D3_IN_ng_per_mL','PRG(ng/mL)':'PRG_IN_ng_per_mL','RBS(mg/dl)':'RBS_IN_mg_per_dl','Pimples(Y/N)':'Pimples','Weight gain(Y/N)': 'Weight_gain','hair growth(Y/N)': 'Hair_Growth','Skin darkening (Y/N)': 'Skin_Darkening','Hair loss(Y/N)': 'Hair_loss','Fast food (Y/N)': 'Fast_food','Reg.Exercise(Y/N)': 'Reg_Exercise','BP _Systolic (mmHg)': 'BP_Systolic_IN_mmHg','BP _Diastolic (mmHg)': 'BP_Diastolic_IN_mmHg','Follicle No. (L)':'Follicle_Num_Left','Follicle No. (R)':'Follicle_Num_Right','Avg. F size (L) (mm)':'Avg_F_size_Left_IN_mm','Avg_F_size(R)_(mm)':'Avg_F_size_Right_IN_mm', 'Endometrium (mm)':'Endometrium_IN_mm'}, inplace=True)

testing_data.columns

training_data.columns

"""CHECKING THE DISTRIBUTION OF FEATURES LIKE WEIGHT AND BMI.

"""

# Plotting the distribution of Item_Weight
training_data.Weight_in_Kg.plot.density(color="Orange")
plt.xlabel("Weight_in_Kg")

# Plotting the distribution of Item_Weight
training_data.BMI.plot.density(color="Orange")
plt.xlabel("BMI")

"""DEALING WITH MISSING DATA SET."""

training_data.isnull().sum()

"""**QUESTION 5:** Which independent variables have missing data? How much? 

**ANSWER 5:**  Percent of Missing Data in Train Data Marriage Status In Years Coloumn = 0.23148148148148145 %

Percent of Missing Data in Train Data II_beta_HCG_IN_mIU_per_mL Coloumn = 0.23148148148148145 %

Percent of Missing Data in Train Data AMH(ng/mL) Size Coloumn = 0.23148148148148145 %

Percent of Missing Data in Train Data Fast_food Size Coloumn = 0.23148148148148145 %
"""

#% of missing data in training dataset
print(
    "Percent of Missing Data in Train Data Marriage Status In Years Coloumn = "
    + str((training_data["MarraigeStatus_In_Yrs"].isnull().sum() / len(training_data)) * 100)
    + " %"
)
print(
    "Percent of Missing Data in Train Data II_beta_HCG_IN_mIU_per_mL Coloumn = "
    + str((training_data["II_beta_HCG_IN_mIU_per_mL"].isnull().sum() / len(training_data)) * 100)
    + " %"
)
print(
    "Percent of Missing Data in Train Data AMH(ng/mL) Size Coloumn = "
    + str((training_data["AMH_IN_ng_per_mL"].isnull().sum() / len(training_data)) * 100)
    + " %"
)
print(
    "Percent of Missing Data in Train Data Fast_food Size Coloumn = "
    + str((training_data["Fast_food"].isnull().sum() / len(training_data)) * 100)
    + " %"
)

"""**QUESTION 1:** What are the data types? (Only numeric and categorical)

**ANSWER 1: **The categorical and numerical variables are segerrated below
"""

numerical_variables = []
categorical_variables = []
for x in training_data.columns:
    if len(training_data[x].value_counts()) < 9:
        if x not in ["RR_in_Breaths_Per_Min", "Cycle", "NumOfAborptions", "BP_Systolic_IN_mmHg", "BP_Diastolic_IN_mmHg"]:
            categorical_variables.append(x)
    else:
        numerical_variables.append(x)

categorical_variables

numerical_variables

"""# DATA IMPUTATION 

I have USED KNN, Median and MICE IMputation Techniques to deal with the missing values of my dataset.

The functions for all the 3 Datasets are written below and have been used again to asnwer the 12th Question.

**Note:** KNN and MICE Imputations techniques are taken reference from the Wine Quality Dataset.
"""

# Initializing objects
encoder = OrdinalEncoder()
imputer = KNN()

# function to encode non-null data and replace it in the original data
def encode(data):
    # retains only non-null values
    nonulls = np.array(data.dropna())
    # reshapes the data for encoding
    impute_reshape = nonulls.reshape(-1, 1)
    # encode date
    impute_ordinal = encoder.fit_transform(impute_reshape)
    # Assign back encoded values to non-null values
    data.loc[data.notnull()] = np.squeeze(impute_ordinal)
    return data

# Function to impute data using KNN method
def knn_impute(training_data):
    for i in categorical_variables:
        encode(training_data[i])
    knn_impute_train_df = pd.DataFrame(
        (imputer.fit_transform(training_data)), columns=training_data.columns
    )
    knn_impute_train_df["Fast_food"] = knn_impute_train_df["Fast_food"].round()
    return knn_impute_train_df

# Performing imputation on training and testing data using KNN imputation
knn_impute_train_df = knn_impute(training_data)
knn_impute_test_df = knn_impute(testing_data)

# Checking if entries are imputed
print(training_data.isnull().sum())
print("\nAfter Imputing the Data\n")
print(knn_impute_train_df.isnull().sum())

# Statistical Difference in MarraigeStatus_In_Yrs after KNN Imputation
training_data.MarraigeStatus_In_Yrs.plot(kind="kde")
knn_impute_train_df.MarraigeStatus_In_Yrs.plot(kind="kde")
plt.legend(["Un - Imputed", "KNN - Imputed"])
plt.xlabel("MarraigeStatus_In_Yrs")

# Statistical Difference in II_beta_HCG_IN_mIU_per_mL after KNN Imputation
training_data.II_beta_HCG_IN_mIU_per_mL.plot(kind="kde")
knn_impute_train_df.II_beta_HCG_IN_mIU_per_mL.plot(kind="kde")
plt.legend(["Un - Imputed", "KNN - Imputed"])
plt.xlabel("II_beta_HCG_IN_mIU_per_mL")

# Statistical Difference in AMH_IN_ng_per_mL after KNN Imputation
training_data.AMH_IN_ng_per_mL.plot(kind="kde")
knn_impute_train_df.AMH_IN_ng_per_mL.plot(kind="kde")
plt.legend(["Un - Imputed", "KNN - Imputed"])
plt.xlabel("AMH_IN_ng_per_mL")

# Statistical Difference in Fast_food after KNN Imputation
training_data.Fast_food.plot(kind="kde")
knn_impute_train_df.Fast_food.plot(kind="kde")
plt.legend(["Un - Imputed", "KNN - Imputed"])
plt.xlabel("Fast_food")

"""MEDIAN METHOD IMPUTATION


"""

median_impute_train_df = training_data.copy() #Making a Copy

#Filling missing values with the median value of the features.
median_impute_train_df['MarraigeStatus_In_Yrs'].fillna(median_impute_train_df['MarraigeStatus_In_Yrs'].median(),inplace=True)
median_impute_train_df['AMH_IN_ng_per_mL'].fillna(median_impute_train_df['AMH_IN_ng_per_mL'].median(),inplace=True)
median_impute_train_df['II_beta_HCG_IN_mIU_per_mL'].fillna(median_impute_train_df['II_beta_HCG_IN_mIU_per_mL'].median(),inplace=True)
median_impute_train_df['Fast_food'].fillna(median_impute_train_df['Fast_food'].median(),inplace=True)

# Checking if Data is Imputed
print(training_data.isnull().sum())
print("\nAfter Imputing the Data\n")
print(median_impute_train_df.isnull().sum())

# Statistical Difference in MarraigeStatus_In_Yrs after KNN Imputationa and Median Imputation
training_data.MarraigeStatus_In_Yrs.plot(kind="kde")
knn_impute_train_df.MarraigeStatus_In_Yrs.plot(kind="kde")
median_impute_train_df.MarraigeStatus_In_Yrs.plot(kind="kde")
plt.legend(["Original", "KNN", "Median"])
plt.xlabel("MarraigeStatus_In_Yrs")

# Statistical Difference in II_beta_HCG_IN_mIU_per_mL after KNN Imputationa and Median Imputation
training_data.II_beta_HCG_IN_mIU_per_mL.plot(kind="kde")
knn_impute_train_df.II_beta_HCG_IN_mIU_per_mL.plot(kind="kde")
median_impute_train_df.II_beta_HCG_IN_mIU_per_mL.plot(kind="kde")
plt.legend(["Original", "KNN", "Median"])
plt.xlabel("II_beta_HCG_IN_mIU_per_mL")

# Statistical Difference in AMH_IN_ng_per_mL after KNN Imputationa and Median Imputation
training_data.AMH_IN_ng_per_mL.plot(kind="kde")
knn_impute_train_df.AMH_IN_ng_per_mL.plot(kind="kde")
median_impute_train_df.AMH_IN_ng_per_mL.plot(kind="kde")
plt.legend(["Original", "KNN", "Median"])
plt.xlabel("AMH_IN_ng_per_mL")

# Statistical Difference in Fast_food after KNN Imputationa and Median Imputation
training_data.Fast_food.plot(kind="kde")
knn_impute_train_df.Fast_food.plot(kind="kde")
median_impute_train_df.Fast_food.plot(kind="kde")
plt.legend(["Original", "KNN", "Median"])
plt.xlabel("Fast_food")

"""Data Imputation - MICE(Multivariate imputation by chained equations) Method"""

# Function to impute data using MICE method
def mice_impute(training_data):
    mice_imputed_train_df = pd.DataFrame(
        MICE().fit_transform(training_data), columns=list(training_data.columns)
    )
    mice_imputed_train_df["Fast_food"] = mice_imputed_train_df[
        "Fast_food"
    ].round()  # Fixing Categorical Values - Outlet Size
    return mice_imputed_train_df

# Performing imputation on training and testing data using MICE imputation
mice_imputed_train_df = mice_impute(training_data)  # Imputing Training Data

# Checking if Data is imputed
print(training_data.isnull().sum())
print("\nAfter Imputing the Data\n")
print(mice_imputed_train_df.isnull().sum())

# Statistical Difference in Item Weight after KNN Imputationa, Mean Mode Imputation and MICE Imputation
training_data.MarraigeStatus_In_Yrs.plot(kind="kde")
knn_impute_train_df.MarraigeStatus_In_Yrs.plot(kind="kde")
median_impute_train_df.MarraigeStatus_In_Yrs.plot(kind="kde")
mice_imputed_train_df.MarraigeStatus_In_Yrs.plot(kind="kde")
plt.legend(["Original", "KNN", "Median", "MICE"])
plt.xlabel("MarraigeStatus_In_Yrs")

# Statistical Difference in Item Weight after KNN Imputationa, Mean Mode Imputation and MICE Imputation
training_data.AMH_IN_ng_per_mL.plot(kind="kde")
knn_impute_train_df.AMH_IN_ng_per_mL.plot(kind="kde")
median_impute_train_df.AMH_IN_ng_per_mL.plot(kind="kde")
mice_imputed_train_df.AMH_IN_ng_per_mL.plot(kind="kde")
plt.legend(["Original", "KNN", "Median", "MICE"])
plt.xlabel("AMH_IN_ng_per_mL")

# Statistical Difference in Item Weight after KNN Imputationa, Mean Mode Imputation and MICE Imputation
training_data.Fast_food.plot(kind="kde")
knn_impute_train_df.Fast_food.plot(kind="kde")
median_impute_train_df.Fast_food.plot(kind="kde")
mice_imputed_train_df.Fast_food.plot(kind="kde")
plt.legend(["Original", "KNN", "Median", "MICE"])
plt.xlabel("Fast_food")

# Statistical Difference in Item Weight after KNN Imputationa, Mean Mode Imputation and MICE Imputation
training_data.II_beta_HCG_IN_mIU_per_mL.plot(kind="kde")
knn_impute_train_df.II_beta_HCG_IN_mIU_per_mL.plot(kind="kde")
median_impute_train_df.II_beta_HCG_IN_mIU_per_mL.plot(kind="kde")
mice_imputed_train_df.II_beta_HCG_IN_mIU_per_mL.plot(kind="kde")
plt.legend(["Original", "KNN", "Median", "MICE"])
plt.xlabel("II_beta_HCG_IN_mIU_per_mL")

"""Using KNN Imputed Dataframe

**QUESTION 7:** In the predictor variables independent of all the other predictor variables?

**ANSWER 7:** HEATMAP HAS BEEN PLOTTED TO SHOW THE CORELATION OF ALL THE FEATURES.

The darker the shade of blue, more the coorealation has been found
"""

plt.figure(figsize=(40, 40))
# plotting correlation heatmap
dataplot = sns.heatmap(knn_impute_train_df.corr(), cmap="YlGnBu", annot=True)
  
# displaying heatmap
plt.show()

#Checking how different features are correlated to PCOS
knn_impute_train_df.corr()["Target"].sort_values(ascending=False) #sorting in descending order

"""DATA EXPLORATION """

#Plotting Box plot for the numerical variables
for y in numerical_variables:
    sns.boxplot(y=y, x="Target", data=knn_impute_train_df)
    plt.show()

#Plotting barplot for categorical variables
for x in categorical_variables:
    if x != "Target":
        sns.barplot(x= x, y = "Target", data = knn_impute_train_df)
        plt.show()

"""Observations from barplot
weight gain, hair growth, pimples, hair loss, fast food and skin darkening means higher chances of PCOS
So the above can also be termed as symptoms of PCOS

CHECKING THE % OF PATIENTS HAVING PCOD FROM THE DATASET.
"""

knn_impute_train_df["Target"].value_counts()

138/(294 + 138)*100

"""31.94 % of patients in our dataset are those having PCOS syndrome

USING LOGISTIC REGRESSION TO TRAIN AND PREDICT ACCURACY
"""

y_train = knn_impute_train_df.iloc[:, 0]
X_train = knn_impute_train_df.iloc[:, 1:]

y_test = knn_impute_test_df.iloc[:, 0]
X_test = knn_impute_test_df.iloc[:, 1:]

from sklearn.linear_model import LogisticRegression
#initializing the object
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

LogisticRegression()

y_pred = logreg.predict(X_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))

from sklearn.metrics import confusion_matrix
print(f"Score in Test Data : {logreg.score(X_test,y_test)}")

cm=confusion_matrix(y_test, y_pred)
p_right=cm[0][0]+cm[1][1]
p_wrong=cm[0][1]+cm[1][0]

print(f"Right classification : {p_right}")
print(f"Wrong classification : {p_wrong}")
cm

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

"""Question 11: Remove outliers and keep outliers (does if have an effect of the final predictive model)?"""

#Plotting Box plot for the numerical variables
for y in numerical_variables:
    sns.boxplot(x=knn_impute_train_df[y])
    # sns.boxplot(y=y, x="Target", data=knn_impute_train_df, width = 0.2)
    plt.show()





#Checking the Range of the Data
plt.figure(figsize=(200,200))
sns.boxplot(data=knn_impute_train_df)

"""QUESTION 9: Do the ranges of the predictor variables make sense

The dataset has been scaled using MIX-MAX SCALING SINCE the range of the dataset was not standardized.

"""

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

sscaler = MinMaxScaler() #helps us scale the dataset. This makes it easy for the model to train
cols = knn_impute_train_df.columns
x_scaled = sscaler.fit_transform(knn_impute_train_df)
X_scaled = pd.DataFrame(x_scaled, columns = cols)
X_scaled

X_scaled_values = X_scaled.to_numpy() #convert the DataFrame to a numpy array
X_scaled_values

plt.figure(figsize=(20,7))
sns.boxplot(data=X_scaled)

X_scaled.head()

y_train = X_scaled.iloc[:, 0]
X_train = X_scaled.iloc[:, 1:]

y_test = X_scaled.iloc[:, 0]
X_test = X_scaled.iloc[:, 1:]

logreg = LogisticRegression()
logreg.fit(X_train, y_train)

LogisticRegression()

y_pred = logreg.predict(X_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))

from sklearn.metrics import confusion_matrix
print(f"Score in Test Data : {logreg.score(X_test,y_test)}")

cm=confusion_matrix(y_test, y_pred)
p_right=cm[0][0]+cm[1][1]
p_wrong=cm[0][1]+cm[1][0]

print(f"Right classification : {p_right}")
print(f"Wrong classification : {p_wrong}")
cm

"""REMOVE OUTLIERS TO CHECK """

# #Plotting Box plot for the numerical variables
# for y in numerical_variables:
#     sns.boxplot(x=X_scaled[y])
#     # sns.boxplot(y=y, x="Target", data=knn_impute_train_df, width = 0.2)
#     plt.show()

# def remove_outliers(df):  # Function to Remove All outliers
#     df_removed_outliers = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]
#     return df_removed_outliers

# X_scaled = remove_outliers(X_scaled)

"""CHECKING STANDARD DEVIATION"""

X_scaled.columns

"""**Question 3:** What are the likely distributions of the numeric variables?

**ANSWER 3:** We can see that features like Weight_in_Kg, BMI etc follow a normal distribution curve. We also have features like I_beta_HCG_IN_mIU_per_mL that follow expon distribution.

(The graph for the same has been plotted below to support Answer 10.)
"""

from scipy.stats import norm
import numpy as np
plt.hist(X_scaled.Age_in_Years, bins=20, rwidth=0.8, density=True)
plt.xlabel('Age_in_Years')

rng = np.arange(X_scaled.Age_in_Years.min(), X_scaled.Age_in_Years.max(), 0.1)
plt.plot(rng, norm.pdf(rng,X_scaled.Age_in_Years.mean(),X_scaled.Age_in_Years.std()))

from scipy.stats import norm
import numpy as np
plt.hist(X_scaled.Weight_in_Kg, bins=20, rwidth=0.8, density=True)
plt.xlabel('Weight_in_Kg')

rng = np.arange(X_scaled.Weight_in_Kg.min(), X_scaled.Weight_in_Kg.max(), 0.1)
plt.plot(rng, norm.pdf(rng,X_scaled.Weight_in_Kg.mean(),X_scaled.Weight_in_Kg.std()))

from scipy.stats import norm
import numpy as np
plt.hist(X_scaled.Height_IN_Cm, bins=20, rwidth=0.8, density=True)
plt.xlabel('Height_IN_Cm')

rng = np.arange(X_scaled.Height_IN_Cm.min(), X_scaled.Height_IN_Cm.max(), 0.1)
plt.plot(rng, norm.pdf(rng,X_scaled.Height_IN_Cm.mean(),X_scaled.Height_IN_Cm.std()))

# Plotting the distribution of Item_Weight
X_scaled.PulseRate_BPM.plot.density(color="Orange")
plt.xlabel("PulseRate_BPM")

# Plotting the distribution of Item_Weight
X_scaled.Hb_IN_g_per_dl.plot.density(color="Orange")
plt.xlabel("Hb_IN_g_per_dl")

# Plotting the distribution of Item_Weight
X_scaled.CycleLength_In_days.plot.density(color="Orange")
plt.xlabel("CycleLength_In_days")

# Plotting the distribution of Item_Weight
X_scaled.MarraigeStatus_In_Yrs.plot.density(color="Orange")
plt.xlabel("MarraigeStatus_In_Yrs")

from scipy.stats import norm
import numpy as np
plt.hist(X_scaled.Waist_IN_inch, bins=20, rwidth=0.8, density=True)
plt.xlabel('Waist_IN_inch')

rng = np.arange(X_scaled.Waist_IN_inch.min(), X_scaled.Waist_IN_inch.max(), 0.1)
plt.plot(rng, norm.pdf(rng,X_scaled.Waist_IN_inch.mean(),X_scaled.Waist_IN_inch.std()))

# Plotting the distribution of Item_Weight
X_scaled.MarraigeStatus_In_Yrs.plot.density(color="Orange")
plt.xlabel("I_beta_HCG_IN_mIU_per_mL")

# Plotting the distribution of Item_Weight
X_scaled.MarraigeStatus_In_Yrs.plot.density(color="Orange")
plt.xlabel("II_beta_HCG_IN_mIU_per_mL")

"""QUESTION 11: Remove outliers and keep outliers (does if have an effect of the final predictive model)?

ANSWER 11: STD DEVIATION HAS BEEN CHECKED AND Z-SCORE VALUE HAS BEEN CALCULATED TO REMOVE OIUTLIERS BASED UPTO 3 STD DEVIATION.

ACCORDING TO MY ANALYSIS, THE OUTLIERS WERE VERT LITTLE TEHREFORE IT DID NOT MAKE ANY IMOACT ON MY ACCURACY.
"""

df['zscore'] = ( X_scaled.Weight_in_Kg -  X_scaled.Weight_in_Kg.mean() ) /  X_scaled.Weight_in_Kg.std()
df.head(5)

X_scaled[df['zscore']>3]

X_scaled[df['zscore']>-3]

#List of All Outliers
X_scaled[(df.zscore>-3) | (df.zscore<3)]

df_no_outliers = X_scaled[(df.zscore>-3) & (df.zscore<3)]
df_no_outliers.head()

df_no_outliers.shape

X_scaled.shape

"""Therefore, we can see that 3 rows have been removed those were the outliers in the DF."""

df_no_outliers.shape[0]

# #Plotting Box plot for the numerical variables
# for y in numerical_variables:
#     sns.boxplot(x=df_no_outliers[y])
#     # sns.boxplot(y=y, x="Target", data=df_no_outliers, width = 0.2)
#     plt.show()

y_train = df_no_outliers.iloc[:, 0]
X_train = df_no_outliers.iloc[:, 1:]

y_test = df_no_outliers.iloc[:, 0]
X_test = df_no_outliers.iloc[:, 1:]

logreg = LogisticRegression()
logreg.fit(X_train, y_train)

LogisticRegression()

y_pred = logreg.predict(X_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))

from sklearn.metrics import confusion_matrix
print(f"Score in Test Data : {logreg.score(X_test,y_test)}")

cm=confusion_matrix(y_test, y_pred)
p_right=cm[0][0]+cm[1][1]
p_wrong=cm[0][1]+cm[1][0]

print(f"Right classification : {p_right}")
print(f"Wrong classification : {p_wrong}")
cm

"""QUESTION 10: What are the distributions of the predictor variables?   


"""

# Plotting the distribution of I_beta_HCG_IN_mIU_per_mL
I_beta_HCG_IN_mIU_per_mL = df_no_outliers["I_beta_HCG_IN_mIU_per_mL"].values
f = Fitter(I_beta_HCG_IN_mIU_per_mL, distributions=get_common_distributions())
f.fit()
print(f.summary())
plt.ylabel("Density")
plt.xlabel("I_beta_HCG_IN_mIU_per_mL")

# Plotting the distribution of Follicle_Num_Right
RBS_IN_mg_per_dl = df_no_outliers["RBS_IN_mg_per_dl"].values
f = Fitter(RBS_IN_mg_per_dl, distributions=get_common_distributions())
f.fit()
print(f.summary())
plt.ylabel("Density")
plt.xlabel("RBS_IN_mg_per_dl")

"""VARIABLE IMPORTANCE PLOT - 
QUESTION : Which independent variables are useful to predict a target (dependent variable)? (Use at least three methods)

ANSWER: I HAVE USED RANDOM FOREST, MUTUAL INFORMTAION AND ELI5 PERMITATION IMPUTATIONS


QUESTION 8 : * Which predictor variables are the most important? IS ALSO ANSWEWED HERE.
"""

# #Random Forest Feature Importance Plot
# def plot_feature_importance(importance,names,model_type):

#   #Create arrays from feature importance and feature names
#   feature_importance = np.array(importance)
#   feature_names = np.array(names)

#   #Create a DataFrame using a Dictionary
#   data={'feature_names':feature_names,'feature_importance':feature_importance}
#   fi_df = pd.DataFrame(data)

#   #Sort the DataFrame in order decreasing feature importance
#   fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)

#   #Define size of bar plot
#   plt.figure(figsize=(10,8))
#   #Plot Searborn bar chart
#   sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])
#   #Add chart labels
#   plt.title(model_type + 'FEATURE IMPORTANCE')
#   plt.xlabel('FEATURE IMPORTANCE')
#   plt.ylabel('FEATURE NAMES')

# plot_feature_importance(logreg.feature_importance,df_no_outliers.columns,'RANDOM FOREST')

from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators=100, max_depth=10)

from sklearn.feature_selection import RFE
# here we want only one final feature, we do this to produce a ranking
n_features_to_select = 1
rfe = RFE(regressor, n_features_to_select=n_features_to_select)
rfe.fit(X_train, y_train)

from operator import itemgetter
features = X_train.columns.to_list()
for x, y in (sorted(zip(rfe.ranking_ , features), key=itemgetter(0))):
    print(x, y)

n_features_to_select = 10
rfe = RFE(regressor, n_features_to_select=n_features_to_select)
rfe.fit(X_train, y_train)

predictions = rfe.predict(X_test)

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import mutual_info_classif

# feature selection
def select_features(X_train, y_train, X_test):
 # configure to select all features
 fs = SelectKBest(score_func=mutual_info_classif, k='all')
 # learn relationship from training data
 fs.fit(X_train, y_train)
 # transform train input data
 X_train_fs = fs.transform(X_train)
 # transform test input data
 X_test_fs = fs.transform(X_test)
 return X_train_fs, X_test_fs, fs

# feature selection
X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)
# what are scores for the features
for i in range(len(fs.scores_)):
 print('Feature %d: %f' % (i, fs.scores_[i]))

# plot the scores

plt.bar([i for i in range(len(fs.scores_))], fs.scores_)
plt.show()

for i, element in enumerate(X_scaled.columns):
    print(i, element)

"""Answer 8

Therefore, we can see that the top 5 features based on Mutual Information Feature Selection are:


1.   Feature 37: 0.223668 - Follicle_Num_Left
2.   Feature 36: 0.220164 - BP_Diastolic_IN_mmHg
3.   Feature 28: 0.155187 - Weight_gain
4.   Feature 29: 0.127932 - Hair_Growth
5.   Feature 27: 0.110684 - RBS_IN_mg_per_dl

"""

pip install eli5

#Understanding the important features 
import eli5
from eli5.sklearn import PermutationImportance
perm = PermutationImportance(logreg, random_state=1).fit(X_test, y_test)
eli5.show_weights(perm, feature_names = X_test.columns.tolist())

"""RANDOMLY REMOVING DATA"""

# Selecting 1000 random rows with no null data points
train_df_selected = df_no_outliers[~df_no_outliers["Age_in_Years"].isnull()]
train_df_selected = train_df_selected.sample(n=100)

train_df_selected.isnull().sum()

"""QUESTION 12: Remove 1%, 5%, and 10% of your data randomly and impute the values back using at least 3 imputation methods. How well did the methods recover the missing values?  That is remove some data, check the % error on residuals for numeric data and check for bias and variance of the error.

For categorical data, calculate the accuracy and a confusion matrix.
"""

# Creating three extra coloumns of Item_Weight for performing Data Imputation techniques
train_df_selected["Item_Weight_1_percent"] = train_df_selected[["Age_in_Years"]]
train_df_selected["Item_Weight_5_percent"] = train_df_selected[["Age_in_Years"]]
train_df_selected["Item_Weight_10_percent"] = train_df_selected[["Age_in_Years"]]

# Function to calculate percentage of missing data in dataset
def get_percent_missing(dataframe):

    percent_missing = dataframe.isnull().sum() * 100 / len(dataframe)
    missing_value_df = pd.DataFrame(
        {"column_name": dataframe.columns, "percent_missing": percent_missing}
    )
    return missing_value_df


# Function to create missing values
def create_missing(dataframe, percent, col):
    dataframe.loc[dataframe.sample(frac=percent).index, col] = np.nan

# Function to calculate how well the data has been recovered after performing data imputation
def percentage_change(l1, l2):
    percent_change = abs(l2 - l1) / (l1 + 0.000000001)
    avg_change = (percent_change.sum() / percent_change.count()) * 100
    return avg_change

print(get_percent_missing(train_df_selected))

# Creating missing values in respected coloumns to perform data imputation
create_missing(train_df_selected, 0.01, "Item_Weight_1_percent")
create_missing(train_df_selected, 0.05, "Item_Weight_5_percent")
create_missing(train_df_selected, 0.1, "Item_Weight_10_percent")

print(get_percent_missing(train_df_selected))

#Performing KNN Imputation
knn_impute_check_df = knn_impute(train_df_selected)

# Double checking the imputation
print(get_percent_missing(knn_impute_check_df))

print(get_percent_missing(train_df_selected))

#Performing Median Imputation
median_impute_check_df = train_df_selected.copy() #Making a Copy

#Filling missing values with the median value of the features.
median_impute_check_df['Item_Weight_1_percent'].fillna(median_impute_check_df['Item_Weight_1_percent'].median(),inplace=True)
median_impute_check_df['Item_Weight_5_percent'].fillna(median_impute_check_df['Item_Weight_5_percent'].median(),inplace=True)
median_impute_check_df['Item_Weight_10_percent'].fillna(median_impute_check_df['Item_Weight_10_percent'].median(),inplace=True)

# Double checking the imputation
print(get_percent_missing(median_impute_check_df))

MICE_impute_check_df = mice_impute(train_df_selected)

# This is the average % error on residuals for 1% missing data imputed using KNN
print("Average % error on residuals for 1% missing data imputed using KNN")
print(
    percentage_change(
        knn_impute_check_df["Age_in_Years"],
        knn_impute_check_df["Item_Weight_1_percent"],
    )
)

# This is the average % error on residuals for 1% missing data imputed using Median
print("Average % error on residuals for 1% missing data imputed using Median")
print(
    percentage_change(
        median_impute_check_df["Age_in_Years"],
        median_impute_check_df["Item_Weight_1_percent"],
    )
)

# This is the average % error on residuals for 1% missing data imputed using MICE Imputation
print("Average % error on residuals for 1% missing data imputed using MICE Imputation")
print(
    percentage_change(
        MICE_impute_check_df["Age_in_Years"],
        MICE_impute_check_df["Item_Weight_1_percent"],
    )
)

# This is the average % error on residuals for 5% missing data imputed using KNN
print("Average % error on residuals for 5% missing data imputed using KNN")
print(
    percentage_change(
        knn_impute_check_df["Age_in_Years"],
        knn_impute_check_df["Item_Weight_5_percent"],
    )
)

# This is the average % error on residuals for 1% missing data imputed using Median
print("Average % error on residuals for 5% missing data imputed using Median")
print(
    percentage_change(
        median_impute_check_df["Age_in_Years"],
        median_impute_check_df["Item_Weight_5_percent"],
    )
)

# This is the average % error on residuals for 5% missing data imputed using MICE Imputation
print("Average % error on residuals for 1% missing data imputed using MICE Imputation")
print(
    percentage_change(
        MICE_impute_check_df["Age_in_Years"],
        MICE_impute_check_df["Item_Weight_5_percent"],
    )
)

# This is the average % error on residuals for 10% missing data imputed using KNN
print("Average % error on residuals for 10% missing data imputed using KNN")
print(
    percentage_change(
        knn_impute_check_df["Age_in_Years"],
        knn_impute_check_df["Item_Weight_10_percent"],
    )
)

# This is the average % error on residuals for 1% missing data imputed using Median
print("Average % error on residuals for 5% missing data imputed using Median")
print(
    percentage_change(
        median_impute_check_df["Age_in_Years"],
        median_impute_check_df["Item_Weight_10_percent"],
    )
)

# This is the average % error on residuals for 5% missing data imputed using MICE Imputation
print("Average % error on residuals for 10% missing data imputed using MICE Imputation")
print(
    percentage_change(
        MICE_impute_check_df["Age_in_Years"],
        MICE_impute_check_df["Item_Weight_10_percent"],
    )
)

"""As we can see, KNN Imputations works best in the above imputation techiques.
------Answer 12
"""

#ANSWER 7
sns.pairplot(knn_impute_train_df)

g = sns.PairGrid(knn_impute_train_df)
g.map(plt.scatter);

"""Conclusion
From the analysis done above on the data set, THE ACCURACY OF THE DATASET IS 91%.

Refernces
Sckit learn offcial documentation

Refered Towards Data Science

Eli5 official documentation

https://www.kaggle.com/code/maheshsaharan/using-logistic-regression-for-pcos-prediction/notebook

https://www.kaggle.com/code/ananyasingh008/pcos-diagnosis#Feature-Selection

WINE- QUALITY AND ABOLONE NOTEBOOKS POSTED ON AI SHUNKS GITHUB.


The algorithms were referred directly from the Sckit learn official documentation. Visualization was referred from the Machine Learning with scikit-learn Quick Start Guide and Towards Data Science 

LOgistic Regression model and scaling models have been refreneced from the Kaggle Notebooks.

The Rest of the code has been refrenced from Notebooks on AI SKUNKS.

Rest of the code, Data Manipulation, Graph Plotting, Variable Importance have been written indpendently.

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"""